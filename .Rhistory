bs(Depth,knots = 10, degree = 3),
data = snow)
snow$pred.spline = predict(lm_spline_cub_int)
snow$resid.spline = lm_spline_cub_int$residuals
AIC(lm_spline_cub_int)
ggplot(site1, aes(x = Depth, y = Density)) + geom_point() +
geom_line(aes(x = Depth, y = pred.spline, group = Core, col = Core), size =1.5) +
geom_smooth() +
theme_bw() +
theme(legend.text = element_text(size = 16),
legend.title = element_text(size = 16),
axis.text.x = element_text(size = 12,angle = 270,hjust = 0,vjust = .5),
axis.text.y = element_text(size = 12),
axis.title = element_text(size = 14)) +
theme(aspect.ratio = 1)
ggplot(site1, aes(x = Depth, y = Density)) + geom_point() +
geom_line(aes(x = Depth, y = pred.spline, group = Core, col = Core), size =1.5) +
geom_smooth() +
theme_bw() +
theme(legend.text = element_text(size = 16),
legend.title = element_text(size = 16),
axis.text.x = element_text(size = 12,angle = 270,hjust = 0,vjust = .5),
axis.text.y = element_text(size = 12),
axis.title = element_text(size = 14)) +
theme(aspect.ratio = 1)
lm_spline_cub_int = lm(Density ~ Core *
bs(Depth,knots = 10, degree = 3),
data = snow)
snow$pred.spline = predict(lm_spline_cub_int)
snow$resid.spline = lm_spline_cub_int$residuals
ggplot(site1, aes(x = Depth, y = Density)) + geom_point() +
geom_line(aes(x = Depth, y = pred.spline, group = Core, col = Core), size =1.5) +
geom_smooth() +
theme_bw() +
theme(legend.text = element_text(size = 16),
legend.title = element_text(size = 16),
axis.text.x = element_text(size = 12,angle = 270,hjust = 0,vjust = .5),
axis.text.y = element_text(size = 12),
axis.title = element_text(size = 14)) +
theme(aspect.ratio = 1)
site1 <- subset(snow, Core == 1)
site1 <- subset(snow, Core == 1)
ggplot(site1, aes(x = Depth, y = Density)) + geom_point() +
geom_line(aes(x = Depth, y = pred.spline, group = Core, col = Core), size =1.5) +
geom_smooth() +
theme_bw() +
theme(legend.text = element_text(size = 16),
legend.title = element_text(size = 16),
axis.text.x = element_text(size = 12,angle = 270,hjust = 0,vjust = .5),
axis.text.y = element_text(size = 12),
axis.title = element_text(size = 14)) +
theme(aspect.ratio = 1)
ggplot(site1, aes(x = Depth, y = resid.spline)) + geom_point() +
geom_smooth() +
theme_bw() +
theme(legend.text = element_text(size = 16),
legend.title = element_text(size = 16),
axis.text.x = element_text(size = 12,angle = 270,hjust = 0,vjust = .5),
axis.text.y = element_text(size = 12),
axis.title = element_text(size = 14)) +
theme(aspect.ratio = 1)
site1 <- subset(snow, Core == 1)
ggplot(site1, aes(x = Depth, y = Density)) + geom_point() +
geom_line(aes(x = Depth, y = pred.linear, group = Core, col = Core), size =1.5) +
theme_bw() +
theme(legend.text = element_text(size = 16),
legend.title = element_text(size = 16),
axis.title = element_text(size = 14)) +
theme(aspect.ratio = 1)
site1 <- subset(snow, Core == 1)
ggplot(site1, aes(x = Depth, y = Density)) + geom_point() +
theme_bw() +
geom_line(aes(x = Depth, y = pred.linear, group = Core, col = Core), size =1.5) +
theme(legend.text = element_text(size = 16),
legend.title = element_text(size = 16),
axis.title = element_text(size = 14)) +
theme(aspect.ratio = 1)
site1 <- subset(snow, Core == 1)
ggplot(site1, aes(x = Depth, y = Density)) + geom_point() +
theme_bw() +
geom_line(aes(x = Depth, y = pred.linear, group = Core, col = Core), size =1.5) +
theme(legend.text = element_text(size = 16),
legend.title = element_text(size = 16),
axis.title = element_text(size = 14)) +
theme(aspect.ratio = 1)
site1 <- subset(snow, Core == 1)
ggplot(site1, aes(x = Depth, y = Density)) +
geom_point() +
theme_bw() +
geom_line(aes(x = Depth, y = pred.linear, group = Core, col = Core), size =1.5) +
theme(legend.text = element_text(size = 16),
legend.title = element_text(size = 16),
axis.title = element_text(size = 14)) +
theme(aspect.ratio = 1)
library(tidyverse)
library(corrplot)  # for the correlation matrix
library(bestglm)  # for variable selection
library(car)  # for VIFs
library(MASS)  # for negative binomial regression
library(pscl)  # for zero-inflated regression
library(gridExtra)
bikes <- read_csv("Bikes.csv") %>%
mutate_if(is.character, as.factor)
bikes$yr <- as.factor(bikes$yr)
summary(bikes)
ggplot(data = bikes) +
geom_histogram(mapping = aes(x = cnt, y = ..density..),
binwidth = 100) +
theme_bw() +
theme(aspect.ratio = 1)
bikes_model <- bestglm(as.data.frame(bikes),
IC = "BIC",
method = "exhaustive",
TopModels = 1,
family = poisson)
summary(bikes_model$BestModel)
bikes_poisson <- glm(cnt ~ season + yr + holiday +
workingday + weathersit + temp + hum + windspeed,
data = bikes,
family = poisson(link = "log"))
summary(bikes_poisson)
#Temperature
ggplot(data = bikes, mapping = aes(y = log(cnt + 1), x = temp)) +
geom_point() +
theme_bw() +
theme(aspect.ratio = 1)
#Humidity
ggplot(data = bikes, mapping = aes(y = log(cnt + 1), x = hum)) +
geom_point() +
theme_bw() +
theme(aspect.ratio = 1)
#Windspeed
ggplot(data = bikes, mapping = aes(y = log(cnt + 1), x = windspeed)) +
geom_point() +
theme_bw() +
theme(aspect.ratio = 1)
# Use added variable plots for any continuous predictors you included in the
# model
avPlots(bikes_poisson, terms = ~ temp + hum + windspeed)
bikes.cooks <- data.frame("cooks.distance" = cooks.distance(bikes_poisson))
bikes.cooks$obs <- 1:nrow(bikes)
ggplot(data = bikes.cooks) +
geom_point(mapping = aes(x = obs, y = abs(cooks.distance))) +
geom_hline(mapping = aes(yintercept = 4/ length(obs)),
color = "red", linetype = "dashed") +  # for n > 30
geom_hline(mapping = aes(yintercept = 1),
color = "red", linetype = "dashed") +  # for n > 30
theme_bw() +
theme(aspect.ratio = 1)
bikes$cooksd <- cooks.distance(bikes_poisson)
bikes %>%
mutate(rowNum = row.names(bikes)) %>%  # save original row numbers
filter(cooksd > 4 / length(cooksd)) %>%  # select potential outliers
arrange(desc(cooksd))
library(tidyverse)
library(ggfortify)  # plot lm objects using ggplot instead of base R
library(car)  # needed for added-variable plots and dfbetas
library(corrplot)  # colored correlation matrix
# install.packages("devtools")
# devtools::install_github("thomasp85/patchwork")
library(patchwork)
FatComplete <- read_table("BodyFat.txt")
bodyfat<- FatComplete %>%
select(-row)
summary(bodyfat)
pairs(bodyfat, pch = 19)
round(cor(bodyfat), 2)
corrplot(cor(bodyfat), type = "upper")
bodyfat_lm <- lm(brozek ~ ., data = bodyfat)
summary(bodyfat_lm)
bodyfat$residuals <- bodyfat_lm$residuals
bodyfat_resid_vs_fit <- autoplot(bodyfat_lm, which = 1, ncol = 1, nrow = 1) +
theme(aspect.ratio = 1)
bodyfat_resid_vs_fit
plot4
## Loading Libraries
library(tidyverse)
library(vroom)
library(DataExplorer)
library(patchwork)
# Load data
bike <- vroom('./train.csv')
library(poissontreg)
install.packages(poissonreg)
install.packages('poissonreg')
library(poissontreg)
library(poissonreg)
pois_mod <- poisson_reg() %>% #Type of model
set_engine("glm") # GLM = generalized linear model
bike_pois_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(pois_mod) %>%
fit(data = trainingSet) # Fit the workflow
bike_pois_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(pois_mod) %>%
fit(data = bike_cleaned) # Fit the workflow
library(tidymodels)
library(vroom)
## Loading Libraries
library(tidyverse)
# Load data
bike <- vroom('./train.csv')
bike_test <- vroom('./test.csv')
setwd('~/College/Stat348/KaggleBikeShare')
getwd()
# Load data
bike <- vroom('./train.csv')
bike_test <- vroom('./test.csv')
# Clean data
bike_cleaned <- bike %>%
select(-'registered', -'casual')
# Feature Engineering
my_recipe <- recipe(count~., data=bike_cleaned) %>% # Set model formula and data
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>% # where weather is 4 change to 3 (only one value like that)
step_date(datetime, features="dow") %>% # gets day of week
step_time(datetime, features="hour") %>% # gets hour
step_dummy(all_nominal_predictors()) #create dummy variables
prepped_recipe <- prep(my_recipe) # Sets up the pre-processing
bake(prepped_recipe, new_data=bike_cleaned)
pois_mod <- poisson_reg() %>% #Type of model
set_engine("glm") # GLM = generalized linear model
# Set up workflow
bike_pois_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(pois_mod) %>%
fit(data = bike_cleaned) # Fit the workflow
bike_predictions <- predict(bike_pois_workflow,
new_data=bike_test) # Use fit to predict12
bike_predictions
bike_predictions_p <- predict(bike_pois_workflow,
new_data=bike_test) # Use fit to predict12
predictions <- bike_predictions_p %>%
mutate(datetime = bike_cleaned$datetime) %>%
mutate(count = ifelse(.pred < 0, 0, .pred)) %>% # round all negative predictions to 0
select(datetime, count)
# Clean data
bike_cleaned <- bike %>%
select(-'registered', -'casual')
# Feature Engineering
my_recipe <- recipe(count~., data=bike_cleaned) %>% # Set model formula and data
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>% # where weather is 4 change to 3 (only one value like that)
step_date(datetime, features="dow") %>% # gets day of week
step_time(datetime, features="hour") %>% # gets hour
step_dummy(all_nominal_predictors()) #create dummy variables
prepped_recipe <- prep(my_recipe) # Sets up the pre-processing
bake(prepped_recipe, new_data=bike_cleaned)
pois_mod <- poisson_reg() %>% #Type of model
set_engine("glm") # GLM = generalized linear model
# Set up workflow
bike_pois_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(pois_mod) %>%
fit(data = bike_cleaned) # Fit the workflow
bike_predictions_p <- predict(bike_pois_workflow,
new_data=bike_test) # Use fit to predict12
predictions <- bike_predictions_p %>%
mutate(datetime = bike_cleaned$datetime) %>%
mutate(count = ifelse(.pred < 0, 0, .pred)) %>% # round all negative predictions to 0
select(datetime, count)
bike_predictions_l <- predict(bike_workflow,
new_data=bike_test) # Use fit to predict on test data
### Linear Regression ###
my_mod <- linear_reg() %>% #Type of model
set_engine("lm") # Engine = What R function to use
#Set up the workflow
bike_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod) %>%
fit(data = bike_cleaned) # Fit the workflow to training data
bike_predictions_l <- predict(bike_workflow,
new_data=bike_test) # Use fit to predict on test data
predictions <- bike_predictions_l %>%
mutate(datetime = bike_test_cleaned$datetime) %>%
mutate(count = ifelse(.pred < 0, 0, .pred)) %>% # round all negative predictions to 0
select(datetime, count)
predictions <- bike_predictions_p %>%
mutate(datetime = bike_test$datetime) %>%
mutate(count = ifelse(.pred < 0, 0, .pred)) %>% # round all negative predictions to 0
select(datetime, count)
predictions$datetime <- as.character(format(predictions$datetime))
vroom_write(predictions, 'BikeSharePreds.csv', delim = ",")
### Linear Regression ###
my_mod <- linear_reg() %>% #Type of model
set_engine("lm") # Engine = What R function to use
#Set up the workflow
bike_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod) %>%
fit(data = bike_cleaned) # Fit the workflow to training data
#### show the model
extract_fit_engine(bike_workflow) %>%
summary()
# Feature Engineering
my_recipe <- recipe(count~., data=bike_cleaned) %>% # Set model formula and data
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>% # where weather is 4 change to 3 (only one value like that)
step_date(datetime, features="dow") %>% # gets day of week
step_time(datetime, features="hour") # gets hour
#step_dummy(all_nominal_predictors()) #create dummy variables
prepped_recipe <- prep(my_recipe) # Sets up the pre-processing
bake(prepped_recipe, new_data=bike_cleaned)
### Linear Regression ###
my_mod <- linear_reg() %>% #Type of model
set_engine("lm") # Engine = What R function to use
#Set up the workflow
bike_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod) %>%
fit(data = bike_cleaned) # Fit the workflow to training data
#### show the model
extract_fit_engine(bike_workflow) %>%
summary()
bike_predictions_l <- predict(bike_workflow,
new_data=bike_test) # Use fit to predict on test data
bike_test
#step_dummy(all_nominal_predictors()) #create dummy variables
step_rm(workingday)
#step_dummy(all_nominal_predictors()) #create dummy variables
step_rm('workingday')
# Feature Engineering
my_recipe <- recipe(count~., data=bike_cleaned) %>% # Set model formula and data
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>% # where weather is 4 change to 3 (only one value like that)
step_date(datetime, features="dow") %>% # gets day of week
step_time(datetime, features="hour") %>% # gets hour
#step_dummy(all_nominal_predictors()) #create dummy variables
step_rm('workingday')
prepped_recipe <- prep(my_recipe) # Sets up the pre-processing
bake(prepped_recipe, new_data=bike_cleaned)
### Linear Regression ###
my_mod <- linear_reg() %>% #Type of model
set_engine("lm") # Engine = What R function to use
#Set up the workflow
bike_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod) %>%
fit(data = bike_cleaned) # Fit the workflow to training data
#### show the model
extract_fit_engine(bike_workflow) %>%
summary()
bike_predictions_l <- predict(bike_workflow,
new_data=bike_test) # Use fit to predict on test data
predictions <- bike_predictions_l %>%
mutate(datetime = bike_test$datetime) %>%
mutate(count = ifelse(.pred < 0, 0, .pred)) %>% # round all negative predictions to 0
select(datetime, count)
predictions$datetime <- as.character(format(predictions$datetime))
### Penalized Regression ###
library(tidymodels)
library(poissonreg) #if you want to do penalized, poisson regression
### Penalized regression model ###
preg_model <- poisson_reg(penalty=0, mixture=0) %>% #Set model and tuning
set_engine("glmnet") # Function to fit in R
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model) %>%
fit(data=bike_cleaned)
## Loading Libraries
library(tidyverse)
library(vroom)
library(tidymodels)
# Load data
bike <- vroom('./train.csv')
bike_test <- vroom('./test.csv')
# Clean data
bike_cleaned <- bike %>%
select(-'registered', -'casual') %>%
mutate(lg_count = log(count)) %>% #Create the log variable of count
select(-count)
# Feature Engineering
my_recipe <- recipe(lg_count~., data=bike_cleaned) %>% # Set model formula and data
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>% # where weather is 4 change to 3 (only one value like that)
step_date(datetime, features="dow") %>% # gets day of week
step_time(datetime, features="hour") %>% # gets hour
step_dummy(all_nominal_predictors()) %>% #create dummy variables
step_normalize(all_numeric_predictors()) %>% # Make mean 0, sd=1
step_rm('workingday', 'datetime')
prepped_recipe <- prep(my_recipe) # Sets up the pre-processing
bake(prepped_recipe, new_data=bike_cleaned)
### Penalized regression model ###
preg_model <- poisson_reg(penalty=0, mixture=0) %>% #Set model and tuning
set_engine("glmnet") # Function to fit in R
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model) %>%
fit(data=bike_cleaned)
bike_pred_penilized <- predict(preg_wf, new_data=bike_test)
# clean format
predictions <- bike_pred_penilized %>%
mutate(datetime = bike_test$datetime) %>%
mutate(count = exp(.pred)) %>% # exponentiate lg_count
select(datetime, count)
predictions$datetime <- as.character(format(predictions$datetime))
vroom_write(predictions, 'BikeSharePreds.csv', delim = ",")
### Penalized regression model ###
preg_model <- linear_reg(penalty=0, mixture=0) %>% #Set model and tuning
set_engine("glmnet") # Function to fit in R
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model) %>%
fit(data=bike_cleaned)
bike_pred_penilized <- predict(preg_wf, new_data=bike_test)
# clean format
predictions <- bike_pred_penilized %>%
mutate(datetime = bike_test$datetime) %>%
mutate(count = exp(.pred)) %>% # exponentiate lg_count
select(datetime, count)
predictions$datetime <- as.character(format(predictions$datetime))
vroom_write(predictions, 'BikeSharePreds.csv', delim = ",")
### Tuning Parameters ###
library(tidymodels)
library(poissonreg) #if you want to do penalized, poisson regression
## Penalized regression model
preg_model <- linear_reg(penalty=tune(),
mixture=tune()) %>% #Set model and tuning
set_engine("glmnet") # Function to fit in R
## Set Workflow
preg_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(preg_model)
## Grid of values to tune over14
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5) ## L^2 total tuning possibilities1
# Load data
bike <- vroom('./train.csv')
## Loading Libraries
library(tidyverse)
library(vroom)
# Load data
bike <- vroom('./train.csv')
bike_test <- vroom('./test.csv')
# Clean data
bike_cleaned <- bike %>%
select(-'registered', -'casual') %>%
mutate(lg_count = log(count)) %>% #Create the log variable of count
select(-count)
# Feature Engineering
my_recipe <- recipe(lg_count~., data=bike_cleaned) %>% # Set model formula and data
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>% # where weather is 4 change to 3 (only one value like that)
step_date(datetime, features="dow") %>% # gets day of week
step_time(datetime, features="hour") %>% # gets hour
step_dummy(all_nominal_predictors()) %>% #create dummy variables
step_normalize(all_numeric_predictors()) %>% # Make mean 0, sd=1
step_rm('workingday', 'datetime')
prepped_recipe <- prep(my_recipe) # Sets up the pre-processing
bake(prepped_recipe, new_data=bike_cleaned)
## Split data for CV
folds <- vfold_cv(bike_cleaned, v = 10, repeats=1)
## Run the CV
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
tuning_grid
folds
## Split data for CV
folds <- vfold_cv(bike_cleaned, v = 5, repeats=1)
## Run the CV
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq)) #Or leave metrics NULL
CV_results
## Plot Results (example)
collect_metrics(CV_results) %>% # Gathers metrics into DF
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
## Find Best Tuning Parameters
bestTune <- CV_results %>%
select_best("rmse")
bestTune
## Run the CV
CV_results <- preg_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae)) #Or leave metrics NULL
tuning_grid
## Plot Results (example)
collect_metrics(CV_results) %>% # Gathers metrics into DF
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
## Finalize the Workflow & fit it
final_wf <- preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=myDataSet)
## Finalize the Workflow & fit it
final_wf <- preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data=bike_cleaned)
## Predict
final_wf %>%
predict(new_data = bike_test)
## Predict
bike_pred_tunned <- final_wf %>%
predict(new_data = bike_test)
# clean format
predictions <- bike_pred_tunned %>%
mutate(datetime = bike_test$datetime) %>%
mutate(count = exp(.pred)) %>% # exponentiate lg_count
select(datetime, count)
predictions$datetime <- as.character(format(predictions$datetime))
vroom_write(predictions, 'BikeSharePreds.csv', delim = ",")
CV_results
folds
tunning_grid
tuning_grid
print(n - 25)
print(n = 25)
tuning_grid
print(n = 25)
